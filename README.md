# 논문, 연구 리뷰
## Generation
 - [ComboGAN] Anoosheh, Asha, et al. "Combogan: Unrestrained scalability for image domain translation." Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2018. / [논문](https://arxiv.org/abs/1712.06909) / [논문리뷰](https://github.com/fud200/Paper-review/blob/main/ComboGAN.pdf)

 - [Sketch-RNN] Ha, David, and Douglas Eck. "A neural representation of sketch drawings." arXiv preprint arXiv:1704.03477 (2017). / [논문](https://arxiv.org/abs/1704.03477) / [논문리뷰](https://github.com/fud200/Paper-review/blob/main/Sketch_RNN.pdf)
 - [Song18] Song, Jifei, et al. "Learning to sketch with shortcut cycle consistency." Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. / [논문](https://arxiv.org/abs/1805.00247) / [논문리뷰](https://github.com/fud200/Paper-review/blob/main/Learning%20to%20Sketch%20with%20Shortcut%20Cycle%20Consistency.pdf)
## Super-Resolution
[EDSR] Lim, Bee, et al. "Enhanced deep residual networks for single image super-resolution." Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2017./ [논문](https://arxiv.org/abs/1707.02921) / [논문리뷰](https://github.com/fud200/Paper-review/blob/main/EDSR.pdf)
[ESRGAN] Wang, Xintao, et al. "Esrgan: Enhanced super-resolution generative adversarial networks." Proceedings of the European conference on computer vision (ECCV) 
workshops. 2018./ [논문](https://arxiv.org/abs/1809.00219) / [논문리뷰](https://github.com/fud200/Paper-review/blob/main/ESRGAN.pdf)
## Vision Transformer(VIT)
[Attention] Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017). / [논문](https://arxiv.org/abs/1706.03762) / [논문리뷰](https://github.com/fud200/Paper-review/blob/main/Attention.pdf)
{VIT_rough_summary  
[Cordonnier19] Cordonnier, Jean-Baptiste, Andreas Loukas, and Martin Jaggi. "On the relationship between self-attention and convolutional layers." arXiv preprint arXiv:1911.03584 (2019)./ [논문](https://arxiv.org/abs/1911.03584)
[Dosovitskiy20] Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers for image recognition at scale." arXiv preprint arXiv:2010.11929 (2020)./ [논문](https://arxiv.org/abs/2010.11929)
[Yuan21] Yuan, Li, et al. "Tokens-to-token vit: Training vision transformers from scratch on imagenet." Proceedings of the IEEE/CVF international conference on computer vision. 2021./ [논문](https://arxiv.org/abs/2101.11986)
[Swin-Transformer] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z.,and Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 10012-10022). / [논문](https://arxiv.org/abs/2103.14030)
[Movile-VIT] Mehta, S., and Rastegari, M. (2021). Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer. arXiv preprint arXiv:2110.02178./ [논문](https://arxiv.org/abs/2110.02178)
[Movile-VIT2] Mehta, S., & Rastegari, M. (2022). Separable self-attention for mobile vision transformers. arXiv preprint arXiv:2206.02680 / [논문](https://arxiv.org/abs/2206.02680)
[Movile-VIT3] Wadekar, S. N., and Chaurasia, A. (2022). Mobilevitv3: Mobile-friendly vision transformer with simple and effective fusion of local, global and input features. arXiv preprint arXiv:2209.15159./ [논문](https://arxiv.org/abs/2209.15159)
} / [논문리뷰](https://github.com/fud200/Paper-review/blob/main/VIT%20Papers%20rough%20summary.pdf)
[Movile-VIT] Mehta, S., and Rastegari, M. (2021). Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer. arXiv preprint arXiv:2110.02178. / [논문](https://arxiv.org/abs/2209.15159) / [논문리뷰](https://github.com/fud200/Paper-review/blob/main/Mobile_ViT.pdf)
[Swin-Transformer] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z.,and Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 10012-10022). / [논문](https://arxiv.org/abs/2103.14030) / [논문리뷰](https://github.com/fud200/Paper-review/blob/main/SWIN%20Transformer.pdf)

## Face Reenactment
{Review of papers about Reenactment  
[FReeNet] Zhang, Jiangning, et al. "Freenet: Multi-identity face reenactment." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. / [논문](https://arxiv.org/abs/1905.11805)
[Huang20] Huang, Po-Hsiang, Fu-En Yang, and Yu-Chiang Frank Wang. "Learning identity-invariant motion representations for cross-id face reenactment." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. / [논문](https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Learning_Identity-Invariant_Motion_Representations_for_Cross-ID_Face_Reenactment_CVPR_2020_paper.pdf)
[Burkov20] Burkov, Egor, et al. "Neural head reenactment with latent pose descriptors." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020. / [논문](https://openaccess.thecvf.com/content_CVPR_2020/papers/Burkov_Neural_Head_Reenactment_with_Latent_Pose_Descriptors_CVPR_2020_paper.pdf)
[MarioNETte] Ha, Sungjoo, et al. "Marionette: Few-shot face reenactment preserving identity of unseen targets." Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 07. 2020. / [논문](https://arxiv.org/abs/1911.08139)
[Zeng20] Zeng, Xianfang, et al. "Realistic face reenactment via self-supervised disentangling of identity and pose." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 07. 2020. / [논문](https://arxiv.org/abs/2003.12957)
[Face2Face] Thies, Justus, et al. "Face2face: Real-time face capture and reenactment of rgb videos." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. / [논문](https://arxiv.org/abs/2007.14808
[x2face] Wiles, Olivia, A. Koepke, and Andrew Zisserman. "X2face: A network for controlling face generation using images, audio, and pose codes." Proceedings of the European conference on computer vision (ECCV). 2018. / [논문](https://arxiv.org/abs/1807.10550)
[ReenactGAN] Wu, Wayne, et al. "Reenactgan: Learning to reenact faces via boundary transfer." Proceedings of the European conference on computer vision (ECCV). 2018./ [논문](https://arxiv.org/abs/1807.11079)
[Ganimation] Pumarola, Albert, et al. "Ganimation: Anatomically-aware facial animation from a single image." Proceedings of the European conference on computer vision (ECCV). 2018. / [논문](https://arxiv.org/abs/1807.09251)
} / [논문리뷰](https://github.com/fud200/Paper-review/blob/main/Face_Reenactment.pdf)

## image inpaiting
[Yu19] Yu, Jiahui, et al. "Free-form image inpainting with gated convolution." Proceedings of the IEEE/CVF international conference on computer vision. 2019./ [논문](https://arxiv.org/abs/1806.03589) / [논문리뷰](https://github.com/fud200/Paper-review/blob/main/Free-Form%20Image%20Inpainting%20with%20Gated%20Convolution.pdf)

## ETC
[Diffusion-Curve] Sun, Xin, et al. "Diffusion curve textures for resolution independent texture mapping." ACM Transactions on Graphics (TOG) 31.4 (2012): 
1-9/ [논문](https://dl.acm.org/doi/10.1145/2185520.2185570) / [논문리뷰](https://github.com/fud200/Paper-review/blob/main/Diffusion_Curve%20%EB%B0%8F%20%EC%97%B0%EA%B5%AC%EB%B0%A9%ED%96%A5.pdf)
